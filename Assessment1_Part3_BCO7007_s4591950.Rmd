---
title: "Assessment1_Part3"
author: "Kris Chavez (s4591950)"
date: '2022-05-18'
output: 
  html_document:
      number_sections: true           
      toc: true                       #TOC = Table of COntents
      toc_depth: 2
      toc_float: true
      theme: cerulean                  #Added a theme to make the document more appealing
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
#install.packages("topicmodels")
#install.packages("tidytext")
#install.packages("ggplot2")
#install.packages("dplyr")
#install.packages("quanteda")
#install.packages("wordcloud")
#install.packages("readtext")
#install.packages("tm")
#install.packages("tidyr")

library(tidyverse)
library(topicmodels)
library(tidytext)
library(ggplot2)
library(dplyr)
library(quanteda)
library(wordcloud)
library(readtext)
library(tm)
library(tidyr)


```

# Part 3: Topic modeling and visualization

## Preprocess text from your dataset to tidy text and convert it to DocumentTermMatrix
```{r}
# Load the dataset
climate_data <- read.csv("climate_tweets.csv", header = TRUE)
head(climate_data) # Lists the top 6 results

# Create a corpus
# Summary: structuring words within the data set 
climate_corpus <- Corpus(VectorSource(climate))

# Text cleaning
climate_data <- tm_map(climate_corpus, content_transformer(tolower)) # Convert the text to lower case
climate_data <- tm_map(climate_corpus, removeNumbers) # Removes numbers
climate_data <- tm_map(climate_corpus, removeWords, stopwords("english")) # Removes English common stop words
climate_data <- tm_map(climate_corpus, removePunctuation) # Removes punctuation
climate_data <- tm_map(climate_corpus, stripWhitespace) # Eliminate extra white spaces

# Create DocumentTermMatrix 
climate_tdm <- DocumentTermMatrix(climate_data)
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), frequency=v)
climate_tdm

# to reduce the sparsity 
removeSparseTerms(climate_tdm, sparse = 0.85)
```


## Use `LDA()` function to create an LDA model. Experiment with different number of topics (`k=`)
```{r}
# Build corpus
climate_corpus <- Corpus(VectorSource(climate))

# Clean the data set
climate_corpus$text <- str_replace_all(climate_corpus$text,"@\\w+", "" )
climate_corpus$text <- str_replace_all(climate_corpus$text,"https://.*\\w+", "")

# Create a document term matrix
climate_tdm <- DocumentTermMatrix(climate_corpus, 
                                  control = list(minWordLength=c(1,Inf)))

# set parameters for Gibbs sampling 
# Gibbs is another sampling tool that draws the instance from the distribution of each variable
burning <- 4000
iter <- 2000
thin <- 500
seed <- list(2003,5,63,100001,756)
nstart <- 5
best <- TRUE

# Number of topics
k <- 2

ldaOut <- LDA(climate_tdm, k, method = "Gibbs", control = list(
  nstart = nstart, 
  seed = seed,
  iter = iter, 
  thin = thin, 
  best = best
))

# set the datadframe 
ldaOut.topics <- as.matrix(topics(ldaOut))

# use the tidy() to transform ldaOut into a matrix
climate_topics <- tidy(ldaOut, matrix = "beta") 

# visualize the dataset
climate_top_terms <- climate_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

climate_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

climate_beta_spread <- climate_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))
  
```


## Visualise the top 10 words from each topic to brainstorm on possible topics that they cover.
```{r}
library(tm)
library(wordcloud)
# Create the wordcloud
wordcloud(d$word,
          d$frequency,
          random.order = FALSE,
          rot.per = 0.3,
          scale = c(4,.5),
          max.words = 10,
          font.main = 1,
          cex.main = 1.5)
wordcloud
```


## Write 2-3 sentences comparing LDA models you generated with different number of topics. Explain which model you think best covers your data. 
As shown from the above plot, by changing the number of k() also known as __'topics'__, this adds new plots to the visualization.  For example, by choosing the topic "k=4" we are expecting to view 4 plots when running the script.  Regarding the model that best fits my particular data set; any topic less than 'k=4' should suffice.  This is due to the sheer amount of keywords within the visualization - adding more topics will make the plots redundant.


## Come to a conclusion about the topics that your dataset presents.
I have come to the conclusion that __Climate, Twitter and Photo__ were the top trending topic words when filtering tweets based on the keyword "#climate".   